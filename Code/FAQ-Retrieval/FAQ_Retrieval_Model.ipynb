{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FAQ-Retrieval-Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "papermill": {
      "duration": 471.658605,
      "end_time": "2020-09-13T16:17:42.994072",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-09-13T16:09:51.335467",
      "version": "2.1.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2GGgtFeuqEM"
      },
      "source": [
        "<center><font size=\"6\" color=\"blue\">Text based Question Answering</font><br>\n",
        "\n",
        "## Objective:\n",
        "To develop a question answering system for closed domain question answering to help provide direct answers from the context or match questions on the fly with FAQ dataset, if a similar question exists. \n",
        "\n",
        "==>We need to give a span of the text as the answer and not the entire paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boqdZ8h8uqEZ",
        "outputId": "9a919ffd-967d-451a-84f9-9dfd887e72ab"
      },
      "source": [
        "!pip install spacy && python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSmm7H9huqE1"
      },
      "source": [
        "from pathlib import Path\n",
        "import os, re, io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "## stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "## lemma functionality provide by NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "#nltk.download('punkt')\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "## cosine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-GmgKn0uqFJ"
      },
      "source": [
        "## <font color=\"#007bff\"><b>Data Loading</b></font><br><a id=\"2\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOGDKEMtuqFO"
      },
      "source": [
        "Prepare the data in the desired format to solve the problem is the next task. As, we need to extract every question and respective answers from an unstructured document and store it structured file. The best and simple way you could extract information from a text file is by doing parsing. Parsing can help to retrieve specific information on the following assumptions.\n",
        "\n",
        "#### Assumptions:\n",
        "\n",
        "- It has to be stored in .xlsx file in first two columns as \"questions\" and \"answers\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXBfRS1uqFi"
      },
      "source": [
        "The data after looks like below,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WBkYqnDXuqFm",
        "outputId": "41e63b8f-4b67-49e0-b664-4db48ec1ea42"
      },
      "source": [
        "path = Path(\"/content/\")\n",
        "in_path = str(path / \"BITSAT-FAQ.csv\")\n",
        "QA_df = pd.read_csv(os.path.join(in_path),header=None)\n",
        "QA_df.columns = ['questions','answers']\n",
        "QA_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I am unable to access the Online application. ...</td>\n",
              "      <td>The application cannot be sent by email/post. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the eligibility for BITSAT?</td>\n",
              "      <td>Candidates can write BITSAT 2020 with Physics,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I have completed the online application but I ...</td>\n",
              "      <td>You can go to the applying online page again a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How will I get back my extra amount if I have ...</td>\n",
              "      <td>If you have made multiple payments towards BIT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can I edit/correct data in my application ...</td>\n",
              "      <td>The link for editing the application form will...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           questions                                            answers\n",
              "0  I am unable to access the Online application. ...  The application cannot be sent by email/post. ...\n",
              "1                What is the eligibility for BITSAT?  Candidates can write BITSAT 2020 with Physics,...\n",
              "2  I have completed the online application but I ...  You can go to the applying online page again a...\n",
              "3  How will I get back my extra amount if I have ...  If you have made multiple payments towards BIT...\n",
              "4  How can I edit/correct data in my application ...  The link for editing the application form will..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "uyF1EvzsuqF7",
        "outputId": "0f87857e-e990-4361-caae-1d2161f5b02d"
      },
      "source": [
        "QA_df = QA_df[:15]\n",
        "QA_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I am unable to access the Online application. ...</td>\n",
              "      <td>The application cannot be sent by email/post. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the eligibility for BITSAT?</td>\n",
              "      <td>Candidates can write BITSAT 2020 with Physics,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I have completed the online application but I ...</td>\n",
              "      <td>You can go to the applying online page again a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How will I get back my extra amount if I have ...</td>\n",
              "      <td>If you have made multiple payments towards BIT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can I edit/correct data in my application ...</td>\n",
              "      <td>The link for editing the application form will...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>My 12th exam results are not expected before 1...</td>\n",
              "      <td>For admissions to I semester 2020-21 starting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I passed 12th in 2019. I didn't get 75% aggreg...</td>\n",
              "      <td>If you are repeating 12th exam, you should do ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the tuition fees and other expenses?</td>\n",
              "      <td>The fee details for 2020-21 are yet to be fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I passed 12th exam in 2019. Am I eligible to a...</td>\n",
              "      <td>As advertised, you are eligible to Apply for B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I had appeared in BITSAT-2019 but my marks wer...</td>\n",
              "      <td>Yes, as advertised. Subject to eligibility con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What is the application procedure for Non Resi...</td>\n",
              "      <td>There is no separate procedure for NRIs. Howev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is the application procedure for candidat...</td>\n",
              "      <td>There is no separate procedure for those candi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>How am I Eligible for admission to M.Sc. progr...</td>\n",
              "      <td>The M.Sc. programmes are 4-year integrated pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>I have passed 12th in the year 2018 (or earlie...</td>\n",
              "      <td>You are NOT eligible to appear in BITSAT-2020.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Does any other university admit students based...</td>\n",
              "      <td>Yes, There are universities which admit studen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            questions                                            answers\n",
              "0   I am unable to access the Online application. ...  The application cannot be sent by email/post. ...\n",
              "1                 What is the eligibility for BITSAT?  Candidates can write BITSAT 2020 with Physics,...\n",
              "2   I have completed the online application but I ...  You can go to the applying online page again a...\n",
              "3   How will I get back my extra amount if I have ...  If you have made multiple payments towards BIT...\n",
              "4   How can I edit/correct data in my application ...  The link for editing the application form will...\n",
              "5   My 12th exam results are not expected before 1...  For admissions to I semester 2020-21 starting ...\n",
              "6   I passed 12th in 2019. I didn't get 75% aggreg...  If you are repeating 12th exam, you should do ...\n",
              "7       What are the tuition fees and other expenses?  The fee details for 2020-21 are yet to be fina...\n",
              "8   I passed 12th exam in 2019. Am I eligible to a...  As advertised, you are eligible to Apply for B...\n",
              "9   I had appeared in BITSAT-2019 but my marks wer...  Yes, as advertised. Subject to eligibility con...\n",
              "10  What is the application procedure for Non Resi...  There is no separate procedure for NRIs. Howev...\n",
              "11  What is the application procedure for candidat...  There is no separate procedure for those candi...\n",
              "12  How am I Eligible for admission to M.Sc. progr...  The M.Sc. programmes are 4-year integrated pro...\n",
              "13  I have passed 12th in the year 2018 (or earlie...     You are NOT eligible to appear in BITSAT-2020.\n",
              "14  Does any other university admit students based...  Yes, There are universities which admit studen..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y0REK8buqGL"
      },
      "source": [
        "## <font color=\"#007bff\"><b>Preprocessing Techniques</b></font><br><a id=\"3\"></a>\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to TOC\">Go to TOC</a>\n",
        "\n",
        "Next step, We will not use the data as it is. Preprocessing is another very important step to fine-tune the dataset.\n",
        "\n",
        "1. Remove unwanted characters\n",
        "2. Remove Question number\n",
        "3. Remove stopwords\n",
        "4. Lemmatization - to reduce inflection of words and minimize the word ambiguity.\n",
        "\n",
        "Why I chosen lemmatization over stemming? Lemmatization is powerful operation as it takes into consideration of morphological analysis of the word. \n",
        "\n",
        "**Example:** bicycles or bicycles are converted to bicyles. But, stemming algorithm works by predefined rules to remove prefix or suffix of the word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8EfWEYZuqGQ"
      },
      "source": [
        "## Data Preprocessing\n",
        "class TextPreprocessor():\n",
        "    def __init__(self, data_df, column_name=None):\n",
        "        self.data_df = data_df  \n",
        "        if not column_name and type(colum_name) == str:\n",
        "            raise Exception(\"column name is mandatory. Make sure type is string format\")\n",
        "        self.column = column_name\n",
        "        self.convert_lowercase()    \n",
        "        self.applied_stopword = False\n",
        "        self.processed_column_name = f\"processed_{self.column}\"\n",
        "        \n",
        "    def convert_lowercase(self):\n",
        "        ## fill empty values into empty\n",
        "        self.data_df.fillna('',inplace=True)\n",
        "        ## reduce all the columns to lowercase\n",
        "        self.data_df = self.data_df.apply(lambda column: column.astype(str).str.lower(), axis=0)    \n",
        "\n",
        "    def remove_question_no(self):\n",
        "        ## remove question no        \n",
        "        self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'^\\d+[.]',' ', row))    \n",
        "        \n",
        "    def remove_symbols(self):\n",
        "        ## remove unwanted character          \n",
        "        self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'[^A-Za-z0-9\\s]', ' ', row))    \n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        ## remove stopwords and create a new column \n",
        "        for idx, question in enumerate(self.data_df[self.column]):      \n",
        "            self.data_df.loc[idx, self.processed_column_name] = remove_stopwords(question)        \n",
        "\n",
        "    def apply_lemmatization(self, perform_stopword):\n",
        "        ## get the root words to reduce inflection of words \n",
        "        lemmatizer = WordNetLemmatizer()    \n",
        "        ## get the column name to perform lemma operation whether stopwords removed text or not\n",
        "        if perform_stopword:\n",
        "            column_name = self.processed_column_name\n",
        "        else:\n",
        "            column_name = self.column\n",
        "        ## iterate every question, perform tokenize and lemma\n",
        "        for idx, question in enumerate(self.data_df[column_name]):\n",
        "\n",
        "            lemmatized_sentence = []\n",
        "            ## use spacy for lemmatization\n",
        "            doc = nlp(question.strip())\n",
        "            for word in doc:       \n",
        "                lemmatized_sentence.append(word.lemma_)      \n",
        "                ## update to the same column\n",
        "                self.data_df.loc[idx, self.processed_column_name] = \" \".join(lemmatized_sentence)\n",
        "\n",
        "    def process(self, perform_stopword = True):\n",
        "        self.remove_question_no()\n",
        "        self.remove_symbols()\n",
        "        if perform_stopword:\n",
        "            self.remove_stopwords()\n",
        "        self.apply_lemmatization(perform_stopword)    \n",
        "        return self.data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "IznJ_SMtuqGj",
        "outputId": "8e97e47a-e47a-4ed1-ff45-a3d41834fe1c"
      },
      "source": [
        "## pre-process training question data\n",
        "text_preprocessor = TextPreprocessor(QA_df.copy(), column_name=\"questions\")\n",
        "processed_QA_df = text_preprocessor.process(perform_stopword=True)\n",
        "processed_QA_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "      <th>processed_questions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i am unable to access the online application  ...</td>\n",
              "      <td>the application cannot be sent by email/post. ...</td>\n",
              "      <td>unable access online application send applicat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what is the eligibility for bitsat</td>\n",
              "      <td>candidates can write bitsat 2020 with physics,...</td>\n",
              "      <td>eligibility bitsat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i have completed the online application but i ...</td>\n",
              "      <td>you can go to the applying online page again a...</td>\n",
              "      <td>complete online application take printout prin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>how will i get back my extra amount if i have ...</td>\n",
              "      <td>if you have made multiple payments towards bit...</td>\n",
              "      <td>extra multiple payment single application conn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>how can i edit correct data in my application ...</td>\n",
              "      <td>the link for editing the application form will...</td>\n",
              "      <td>edit correct datum application form</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>my 12th exam results are not expected before 1...</td>\n",
              "      <td>for admissions to i semester 2020-21 starting ...</td>\n",
              "      <td>12th exam result expect 18th june 2020 apply b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i passed 12th in 2019  i didn t get 75  aggreg...</td>\n",
              "      <td>if you are repeating 12th exam, you should do ...</td>\n",
              "      <td>pass 12th 2019 t 75 aggregate pcm eligible rep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>what are the tuition fees and other expenses</td>\n",
              "      <td>the fee details for 2020-21 are yet to be fina...</td>\n",
              "      <td>tuition fee expense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>i passed 12th exam in 2019  am i eligible to a...</td>\n",
              "      <td>as advertised, you are eligible to apply for b...</td>\n",
              "      <td>pass 12th exam 2019 eligible appear bitsat 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>i had appeared in bitsat 2019 but my marks wer...</td>\n",
              "      <td>yes, as advertised. subject to eligibility con...</td>\n",
              "      <td>appear bitsat 2019 mark cut mark appear bitsat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           questions  ...                                processed_questions\n",
              "0  i am unable to access the online application  ...  ...  unable access online application send applicat...\n",
              "1                what is the eligibility for bitsat   ...                                 eligibility bitsat\n",
              "2  i have completed the online application but i ...  ...  complete online application take printout prin...\n",
              "3  how will i get back my extra amount if i have ...  ...  extra multiple payment single application conn...\n",
              "4  how can i edit correct data in my application ...  ...                edit correct datum application form\n",
              "5  my 12th exam results are not expected before 1...  ...  12th exam result expect 18th june 2020 apply b...\n",
              "6  i passed 12th in 2019  i didn t get 75  aggreg...  ...  pass 12th 2019 t 75 aggregate pcm eligible rep...\n",
              "7      what are the tuition fees and other expenses   ...                                tuition fee expense\n",
              "8  i passed 12th exam in 2019  am i eligible to a...  ...    pass 12th exam 2019 eligible appear bitsat 2020\n",
              "9  i had appeared in bitsat 2019 but my marks wer...  ...  appear bitsat 2019 mark cut mark appear bitsat...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLzHKaDZ3f09"
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQwJtwV4uqG5"
      },
      "source": [
        "## <font color=\"#007bff\"><b>Techniques for Question representations</b></font><br><a id=\"4\"></a>\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to TOC\">Go to TOC</a>\n",
        "\n",
        "In this section will be discussing on multiple ways of representing FAQ questions.\n",
        "\n",
        "1. TF-IDF\n",
        "2. Word Embedding\n",
        "3. BERT Embedding\n",
        "4. Sentence BERT (SBERT) Embedding \n",
        "\n",
        "### <font color=\"#007bff\"><b>1. TF_IDF Representation</b></font><br><a id=\"4.1\"></a>\n",
        "\n",
        "The first approach we will use for semantic similarity is leveraging Bag of Words (BOW). TF-IDF transforms the text into meaningful numbers. The technique is a widely used feature extraction in NLP applications. TF (Term Frequency) measures the no of times that words appear in a document. IDF (Inverse Document Frequency) measures low value for words that has high frequency across all the documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zemqe2F7uqG8"
      },
      "source": [
        "class TF_IDF():\n",
        "    def __init__(self):\n",
        "        self.dictionary = None    \n",
        "        self.model = None\n",
        "        self.bow_corpus = None\n",
        "\n",
        "    def create_tf_idf_model(self, data_df, column_name):\n",
        "        ## create sentence token list\n",
        "        sentence_token_list = [sentence.split(\" \") for sentence in data_df[column_name]]\n",
        "\n",
        "        ## dataset vocabulary\n",
        "        self.dictionary = Dictionary(sentence_token_list) \n",
        "\n",
        "        ## bow representation of dataset\n",
        "        self.bow_corpus = [self.dictionary.doc2bow(sentence_tokens) for sentence_tokens in sentence_token_list]\n",
        "\n",
        "        ## compute TF-IDF score for corpus\n",
        "        self.model = TfidfModel(self.bow_corpus)\n",
        "\n",
        "        ## representation of question and respective TF-IDF value\n",
        "        print(f\"First 10 question representation of TF-IDF vector\")\n",
        "        for index, sentence in enumerate(data_df[column_name]):\n",
        "            if index <= 10:\n",
        "                print(f\"{sentence} {self.model[self.bow_corpus[index]]}\")\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def get_vector_for_test_set(self, test_df, column_name):\n",
        "        ## store tf-idf vector\n",
        "        testset_tf_idf_vector = []\n",
        "        sentence_token_list = [sentence.split(\" \") for sentence in test_df[column_name]]\n",
        "        test_bow_corpus = [self.dictionary.doc2bow(sentence_tokens) for sentence_tokens in sentence_token_list]   \n",
        "        for test_sentence in test_bow_corpus:\n",
        "            testset_tf_idf_vector.append(self.model[test_sentence])      \n",
        "\n",
        "        return testset_tf_idf_vector\n",
        "\n",
        "    def get_training_QA_vectors(self):\n",
        "        QA_vectors = []\n",
        "        for sentence_vector in self.bow_corpus:\n",
        "            QA_vectors.append(self.model[sentence_vector])      \n",
        "        return QA_vectors\n",
        "\n",
        "    def get_train_vocabulary(self):\n",
        "        vocab = []\n",
        "        for index in self.dictionary:\n",
        "            vocab.append(self.dictionary[index])\n",
        "        return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-B__hcVuqHO"
      },
      "source": [
        "### <font color=\"#007bff\"><b>2. Word Embedding</b></font><br><a id=\"4.2\"></a>\n",
        "\n",
        "*GloVe* is an unsupervised learning algorithm for obtaining vector representations for words. It trained on the global word-word co-occurrence matrix. I downloaded a pre-trained word vector from Glove for our analysis. The code snippets for generating word embedding representation as below code snippet,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di7YrY0zuqHS"
      },
      "source": [
        "class Embeddings():\n",
        "    def __init__(self, model_path):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.__load_model__()\n",
        "        \n",
        "    def __load_model__(self):\n",
        "        #word_vectors = api.load(\"glove-wiki-gigaword-100\")  \n",
        "        model_name = 'glove-twitter-25' #'word2vec-google-news-50' #'glove-twitter-25'  \n",
        "        if not os.path.exists(self.model_path+ model_name):\n",
        "            print(\"Downloading model\")\n",
        "            self.model = api.load(model_name)\n",
        "            self.model.save(self.model_path+ model_name)\n",
        "        else:\n",
        "            print(\"Loading model from Drive\")\n",
        "            self.model = KeyedVectors.load(self.model_path+ model_name)\n",
        "        \n",
        "    def get_oov_from_model(self, document_vocabulary):\n",
        "        ## the below words are not available in our pre-trained model model_name\n",
        "        print(\"The below words are not found in our pre-trained model\")\n",
        "        words = []\n",
        "        for word in set(document_vocabulary):  \n",
        "            if word not in self.model:\n",
        "                words.append(word)\n",
        "        print(words)  \n",
        "\n",
        "    def get_sentence_embeddings(self, data_df, column_name):\n",
        "        sentence_embeddings_list = []\n",
        "        for sentence in data_df[column_name]:      \n",
        "            sentence_embeddings = np.repeat(0, self.model.vector_size)\n",
        "            try:\n",
        "                tokens = sentence.split(\" \")\n",
        "                ## get the word embedding\n",
        "                for word in tokens:\n",
        "                    if word in self.model:\n",
        "                        word_embedding = self.model[word]\n",
        "                    else:\n",
        "                        word_embedding = np.repeat(0, self.model.vector_size)          \n",
        "                    sentence_embeddings = sentence_embeddings + word_embedding\n",
        "                ## take the average for sentence embeddings\n",
        "                #sentence_embeddings = sentence_embeddings / len(tokens)\n",
        "                sentence_embeddings_list.append(sentence_embeddings.reshape(1, -1))\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "            \n",
        "        return sentence_embeddings_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Z7XU13uqHg"
      },
      "source": [
        "### <font color=\"#007bff\"><b>3. BERT Embedding</b></font><br><a id=\"4.3\"></a>\n",
        "\n",
        "*BERT* is a transformer-based model attempts to use the context of words to get embedding. BERT broke several records in NLP tasks. \n",
        "\n",
        "The following search query is an excellent way to understand BERT. \n",
        "> “2019 Brazil traveler to the USA need a visa”. \n",
        "\n",
        "We observe that the relationship of the word “to” to other words in the sentence are important to decode the meaning semantically. Returning information about USA citizens traveling to Brazil is not relevant since we are talking about Brazil citizens traveling to the USA. BERT can handle this well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "bXyUvzxQuqHk",
        "outputId": "ba560d95-5c1e-4411-b743-884d94ebaa02"
      },
      "source": [
        "!pip install bert-embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-embedding\n",
            "  Downloading https://files.pythonhosted.org/packages/62/85/e0d56e29a055d8b3ba6da6e52afe404f209453057de95b90c01475c3ff75/bert_embedding-1.0.1-py3-none-any.whl\n",
            "Collecting gluonnlp==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/07/037585c23bccec19ce333b402997d98b09e43cc8d2d86dc810d57249c5ff/gluonnlp-0.6.0.tar.gz (209kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 7.3MB/s \n",
            "\u001b[?25hCollecting numpy==1.14.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 328kB/s \n",
            "\u001b[?25hCollecting typing==3.6.6\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
            "Collecting mxnet==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/e9/241aadccc4522f99adee5b6043f730d58adb7c001e0a68865a3728c3b4ae/mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6MB)\n",
            "\u001b[K     |████████████████████████████████| 29.6MB 152kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.4.0->bert-embedding) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-cp36-none-any.whl size=259917 sha256=8d834a47a5c23ee3fa681d38183314b94acde1d7ab7536230762b8fabba8d4c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/48/ac/a77c79aa416ba6dd7bf487f2280b0471034f66141617965914\n",
            "Successfully built gluonnlp\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.4 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.1 has requirement numpy>=1.16, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, gluonnlp, typing, graphviz, mxnet, bert-embedding\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRdKDkVnuqHz"
      },
      "source": [
        "from bert_embedding import BertEmbedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lKGAwPWuqH9"
      },
      "source": [
        "## get bert embeddings\n",
        "def get_bert_embeddings(sentences):\n",
        "    bert_embedding = BertEmbedding()\n",
        "    return bert_embedding(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntFEJpML1Qu8"
      },
      "source": [
        "### <font color=\"#007bff\"><b>4. Sentence BERT (SBERT) Embedding</b></font><br><a id=\"4.3\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyBQ6amd1XAR",
        "outputId": "deea13e4-eb55-4076-f49c-c8c6e952cbb0"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/5a/6e41e8383913dd2ba923cdcd02be2e03911595f4d2f9de559ecbed80d2d3/sentence-transformers-0.3.9.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hCollecting transformers<3.6.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.9-cp36-none-any.whl size=101036 sha256=29a629b1bf1c782c2fd505b4edac98b1afe54f048c2efdc61e87e2c968289a4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/89/43/f2f5bc00b03ef9724b0f6254a97eaf159a4c4ddc024b33e07a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ca4aa62cc322d493fddecf16af8150a9070fc4ae4d9cc9a4fe8a1afa3559002c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.9 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNuPxm531mme",
        "outputId": "4a65e2f7-4175-4095-aa41-23418a0cc816"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:18<00:00, 21.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfAY4A4-2nNO"
      },
      "source": [
        "## get sbert embeddings\n",
        "def get_sbert_embeddings(sentences):\n",
        "    return sbert_model.encode(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMkS7-3HuqIJ"
      },
      "source": [
        "## <font color=\"#007bff\"><b>Encoding and Analysis using above techniques</b></font><br><a id=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feWE1hjjuqIK"
      },
      "source": [
        "#### **1. TF-IDF Computation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Jsus8juqIM",
        "outputId": "6e42d7cd-5a98-42b4-b412-b94972332cbb"
      },
      "source": [
        "tf_idf = TF_IDF()\n",
        "tf_idf.create_tf_idf_model(processed_QA_df, \"processed_questions\")\n",
        "## get the tf-idf reprentation \n",
        "question_QA_vectors = tf_idf.get_training_QA_vectors()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 question representation of TF-IDF vector\n",
            "unable access online application send application form email post [(0, 0.39028148276826063), (1, 0.2641098051558333), (2, 0.39028148276826063), (3, 0.2903858053190506), (4, 0.2903858053190506), (5, 0.39028148276826063), (6, 0.39028148276826063), (7, 0.39028148276826063)]\n",
            "eligibility bitsat [(8, 0.32050826552749295), (9, 0.9472457187702449)]\n",
            "complete online application take printout printout [(1, 0.131031595749367), (4, 0.28813557627267694), (10, 0.38725715198934396), (11, 0.7745143039786879), (12, 0.38725715198934396)]\n",
            "extra multiple payment single application connection error [(1, 0.1368348114310468), (13, 0.4044082579070788), (14, 0.4044082579070788), (15, 0.4044082579070788), (16, 0.4044082579070788), (17, 0.4044082579070788), (18, 0.4044082579070788)]\n",
            "edit correct datum application form [(1, 0.176667599103797), (3, 0.388488136661716), (19, 0.5221320162245074), (20, 0.5221320162245074), (21, 0.5221320162245074)]\n",
            "12th exam result expect 18th june 2020 apply bitsat 2020 [(8, 0.13762378971245434), (22, 0.1650078750463052), (23, 0.4067400426990747), (24, 0.3300157500926104), (25, 0.3026316647587595), (26, 0.3026316647587595), (27, 0.4067400426990747), (28, 0.4067400426990747), (29, 0.4067400426990747)]\n",
            "pass 12th 2019 t 75 aggregate pcm eligible repeat subject require 75 mark [(22, 0.11821032657871258), (30, 0.17317499831323252), (31, 0.5827706497838903), (32, 0.29138532489194513), (33, 0.17317499831323252), (34, 0.21680291267403648), (35, 0.17317499831323252), (36, 0.29138532489194513), (37, 0.29138532489194513), (38, 0.29138532489194513), (39, 0.29138532489194513), (40, 0.29138532489194513)]\n",
            "tuition fee expense [(41, 0.5773502691896257), (42, 0.5773502691896257), (43, 0.5773502691896257)]\n",
            "pass 12th exam 2019 eligible appear bitsat 2020 [(8, 0.20941927612850858), (22, 0.25108907275333864), (24, 0.25108907275333864), (26, 0.4605083488818472), (30, 0.3678388429252582), (33, 0.3678388429252582), (35, 0.3678388429252582), (44, 0.4605083488818472)]\n",
            "appear bitsat 2019 mark cut mark appear bitsat 2020 [(8, 0.26740108438609667), (24, 0.1603039881833547), (30, 0.2348410979540941), (34, 0.588009060752806), (44, 0.588009060752806), (45, 0.3951450861374488)]\n",
            "application procedure non resident indians nris [(1, 0.15660557578445627), (46, 0.462839736607824), (47, 0.462839736607824), (48, 0.462839736607824), (49, 0.34437219182219014), (50, 0.462839736607824)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvcxRmituqIX"
      },
      "source": [
        "## Get the document vocabulary list from TF-IDF\n",
        "document_vocabulary = tf_idf.get_train_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbXYroQluqIl"
      },
      "source": [
        "#### **2. Embeddings (Glove)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB0RTnmXuqIr"
      },
      "source": [
        "## Now, Let's try building embedding based\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux9SxJtfuqI7",
        "outputId": "746cb9a5-aac3-42fe-b5ac-be19f6f6eb1c"
      },
      "source": [
        "## create Embedding object\n",
        "embedding = Embeddings(\"\")\n",
        "## look for out of vocabulary COVID QA dataset - pretrained model\n",
        "embedding.get_oov_from_model(document_vocabulary)\n",
        "## get the sentence embedding for COVID QA dataset\n",
        "question_QA_embeddings = embedding.get_sentence_embeddings(processed_QA_df, \"processed_questions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading model\n",
            "[===============================================---] 94.4% 99.0/104.8MB downloaded"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-135668d2eb18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## create Embedding object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m## look for out of vocabulary COVID QA dataset - pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_oov_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_vocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## get the sentence embedding for COVID QA dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-15517a48756b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load_model__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__load_model__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-15517a48756b>\u001b[0m in \u001b[0;36m__load_model__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_progress\u001b[0;34m(chunks_downloaded, chunk_size, total_size, part, total_parts)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 round(float(total_size) / (1024 * 1024), 1))\n\u001b[1;32m     97\u001b[0m         )\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         sys.stdout.write(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouting_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \"\"\"Send a single zmq message frame on this socket.\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcFDH3mJuqJN"
      },
      "source": [
        "#### **3. BERT Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PcBaQl2uqJR",
        "outputId": "3044f6b0-738f-459c-e90b-1ae854376f99"
      },
      "source": [
        "question_QA_bert_embeddings_list = get_bert_embeddings(processed_QA_df[\"questions\"].to_list())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab file is not found. Downloading.\n",
            "Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
            "Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRHmomi-v8Dn"
      },
      "source": [
        "#### **4. Sentence BERT (SBERT) Embedding**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8leI2cIqwox2"
      },
      "source": [
        "sentences = processed_QA_df[\"questions\"].to_list()\n",
        "question_QA_sbert_embeddings = get_sbert_embeddings(sentences)\n",
        "\n",
        "# print('Sample BERT embedding vector - length', len(question_QA_sbert_embeddings_list[1]))\n",
        "# print('Sample BERT embedding vector - note includes negative values', question_QA_sbert_embeddings_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_15ui0U8uqJk"
      },
      "source": [
        "## <font color=\"#007bff\"><b>Evaluating with test queries</b></font><br><a id=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flyjvCtKuqJn"
      },
      "source": [
        "Utility for evaluating user test query.\n",
        "\n",
        "One of the best techniques to find a similarity score is **Cosine Similarity**. We will use cosine similarity for comparing each representation now. How to calculate cosine similarity as below code snippet,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yScOebpeuqJq"
      },
      "source": [
        "## helps to retrieve similar question based of input vectors/embeddings for test query\n",
        "def retrieveSimilarFAQ(train_question_vectors, test_question_vectors, train_QA_df, train_column_name, test_QA_df, test_column_name):\n",
        "    similar_question_index = []\n",
        "    for test_index, test_vector in enumerate(test_question_vectors):\n",
        "        sim, sim_Q_index = -1, -1\n",
        "        for train_index, train_vector in enumerate(train_question_vectors):\n",
        "            sim_score = cosine_similarity(train_vector, test_vector)[0][0]\n",
        "            \n",
        "            if sim < sim_score:\n",
        "                sim = sim_score\n",
        "                sim_Q_index = train_index\n",
        "\n",
        "        print(\"######\")\n",
        "        print(f\"Query Question: {test_QA_df[test_column_name].iloc[test_index]}\")    \n",
        "        print(f\"Retrieved Question: {train_QA_df[train_column_name].iloc[sim_Q_index]}\")\n",
        "        print(\"######\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DTSPlwyuqJ4"
      },
      "source": [
        "Let's create sample few question for testing purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkE7nIENuqJ8"
      },
      "source": [
        "test_query_string = [\"How to know if I am eligible to apply for BITSAT?\",\n",
        "                    \"Can I get the application form via mail?\",\n",
        "                    \"What are the fees?\",\n",
        "                    \"Can I appear for BITSAT 2020 if I also took BITSAT 2019?\",\n",
        "                    \"Can I appear if I failed 12th\"]\n",
        "\n",
        "# test_query_string = [\"how does covid-19 spread?\", \n",
        "#                      \"What are the symptoms of COVID-19?\",\n",
        "#                     \"Should I wear a mask to protect myself from covid-19\",              \n",
        "#                     \"Is there a vaccine for COVID-19\",\n",
        "#                     \"can the virus transmit through air?\",\n",
        "#                     \"can the virus spread through air?\"]\n",
        "\n",
        "#test_query_string = [\"Is it required to have background in  algorithms and complexity for data scientist roles\"]\n",
        "\n",
        "test_QA_df = pd.DataFrame(test_query_string, columns=[\"test_questions\"])              \n",
        "## pre-process testing QA data\n",
        "text_preprocessor = TextPreprocessor(test_QA_df, column_name=\"test_questions\")\n",
        "query_QA_df = text_preprocessor.process(perform_stopword=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "72B_I4mBuqKM",
        "outputId": "639e88a9-92b7-43ab-93e7-36b44f739260"
      },
      "source": [
        "## TF-IDF vector represetation\n",
        "query_QA_vectors = tf_idf.get_vector_for_test_set(query_QA_df, \"processed_test_questions\")\n",
        "query_QA_df.head()\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d6a2a2ed02d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## TF-IDF vector represetation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery_QA_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector_for_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_QA_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"processed_test_questions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mquery_QA_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf_idf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlVqCG56uqKY"
      },
      "source": [
        "### **Test with TF-IDF computation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrhl6vH9uqKb",
        "outputId": "41f6369c-1465-479b-fe8a-1c56b349278b"
      },
      "source": [
        "retrieveSimilarFAQ(question_QA_vectors, query_QA_vectors, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######\n",
            "Query Question: how does covid 19 spread \n",
            "Retrieved Question:   how does covid 19 spread \n",
            "######\n",
            "######\n",
            "Query Question: what are the symptoms of covid 19 \n",
            "Retrieved Question:   what are the symptoms of covid 19 \n",
            "######\n",
            "######\n",
            "Query Question: should i wear a mask to protect myself from covid 19\n",
            "Retrieved Question:   can i catch covid 19 from my pet\n",
            "######\n",
            "######\n",
            "Query Question: is there a vaccine for covid 19\n",
            "Retrieved Question:   should i worry about covid 19 \n",
            "######\n",
            "######\n",
            "Query Question: can the virus transmit through air \n",
            "Retrieved Question:   can i catch covid 19 from my pet\n",
            "######\n",
            "######\n",
            "Query Question: can the virus spread through air \n",
            "Retrieved Question:   what is community spread \n",
            "######\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwd_NrxfuqKj"
      },
      "source": [
        "### **Test with Embeddings (glove-twitter-25)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NV7tLZJuqKl",
        "outputId": "4a0c1809-465d-42ea-b916-a54c9f84a726"
      },
      "source": [
        "## get the sentence embedding for COVID QA query\n",
        "query_QA_embeddings = embedding.get_sentence_embeddings(query_QA_df, \"processed_test_questions\")\n",
        "\n",
        "retrieveSimilarFAQ(question_QA_embeddings, query_QA_embeddings, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######\n",
            "Query Question: how does covid 19 spread \n",
            "Retrieved Question:   how does covid 19 spread \n",
            "######\n",
            "######\n",
            "Query Question: what are the symptoms of covid 19 \n",
            "Retrieved Question:   what are the symptoms of covid 19 \n",
            "######\n",
            "######\n",
            "Query Question: should i wear a mask to protect myself from covid 19\n",
            "Retrieved Question:   should i wear a mask to protect myself from catching the covid 19 virus \n",
            "######\n",
            "######\n",
            "Query Question: is there a vaccine for covid 19\n",
            "Retrieved Question:   is there a vaccine  drug or treatment for covid 19 \n",
            "######\n",
            "######\n",
            "Query Question: can the virus transmit through air \n",
            "Retrieved Question:   can the virus that causes covid 19 be transmitted through the air \n",
            "######\n",
            "######\n",
            "Query Question: can the virus spread through air \n",
            "Retrieved Question:   can the virus that causes covid 19 be transmitted through the air \n",
            "######\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPZDc65yuqKw"
      },
      "source": [
        "### **Test with BERT Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMrNZ79IuqKy"
      },
      "source": [
        "query_QA_bert_embeddings_list = get_bert_embeddings(test_QA_df[\"test_questions\"].to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfs5IW1wuqK-"
      },
      "source": [
        "## store QA bert embeddings in list\n",
        "question_QA_bert_embeddings = []\n",
        "for embeddings in question_QA_bert_embeddings_list:\n",
        "    question_QA_bert_embeddings.append(embeddings[1])\n",
        "\n",
        "## store query string bert embeddings in list\n",
        "query_QA_bert_embeddings = []\n",
        "for embeddings in query_QA_bert_embeddings_list:\n",
        "    query_QA_bert_embeddings.append(embeddings[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWyH0qx5uqLN",
        "outputId": "abef683d-638c-42dc-d857-2c566db127de"
      },
      "source": [
        "retrieveSimilarFAQ(question_QA_bert_embeddings, query_QA_bert_embeddings, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######\n",
            "Query Question: how does covid 19 spread \n",
            "Retrieved Question:   how does covid 19 spread \n",
            "######\n",
            "######\n",
            "Query Question: what are the symptoms of covid 19 \n",
            "Retrieved Question:   what are the symptoms of covid 19 \n",
            "######\n",
            "######\n",
            "Query Question: should i wear a mask to protect myself from covid 19\n",
            "Retrieved Question:   should i wear a mask to protect myself from catching the covid 19 virus \n",
            "######\n",
            "######\n",
            "Query Question: is there a vaccine for covid 19\n",
            "Retrieved Question:   is there a vaccine  drug or treatment for covid 19 \n",
            "######\n",
            "######\n",
            "Query Question: can the virus transmit through air \n",
            "Retrieved Question:   can the virus that causes covid 19 be transmitted through the air \n",
            "######\n",
            "######\n",
            "Query Question: can the virus spread through air \n",
            "Retrieved Question:   can the virus that causes covid 19 be transmitted through the air \n",
            "######\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzvAhQcLxxEy"
      },
      "source": [
        "### **Test with Sentence BERT Embeddings (SBERT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-3Jm1-IyG8o"
      },
      "source": [
        "query_QA_sbert_embeddings_list = sbert_model.encode(test_query_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mItAoKpZ7Qjj"
      },
      "source": [
        "def evaluate_sbert(question_sbert_embeddings, query_sbert_embeddings, train_QA_df, train_column_name, test_QA_df, test_column_name):\n",
        "\n",
        "  for test_index, test_vector in enumerate(query_sbert_embeddings):\n",
        "        sim, sim_Q_index, sim2, sim_Q_index2 = -1, -1, -1, -1\n",
        "        for train_index, train_vector in enumerate(question_sbert_embeddings):\n",
        "            sim_score = cosine(train_vector, test_vector)\n",
        "            \n",
        "            if sim < sim_score:\n",
        "                sim2 = sim\n",
        "                sim_Q_index2 = sim_Q_index\n",
        "                sim = sim_score\n",
        "                sim_Q_index = train_index\n",
        "\n",
        "            elif sim2 < sim_score:\n",
        "              sim2 = sim_score\n",
        "              sim_Q_index2 = train_index\n",
        "              \n",
        "        query = test_QA_df[test_column_name].iloc[test_index]\n",
        "        retrieved_ques = train_QA_df[train_column_name].iloc[sim_Q_index]\n",
        "\n",
        "        # to print query and corresponding retrieved question\n",
        "        print(\"######\")\n",
        "        print(f\"Query Question: {query}\")    \n",
        "        print(f\"Retrieved Question 1: {retrieved_ques}\")\n",
        "        # print(f\"Retrieved Question 2: {train_QA_df[train_column_name].iloc[sim_Q_index2]}\")\n",
        "        print(\"######\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9bYfUvuy-rK",
        "outputId": "0c3724f1-7494-47d0-b77a-eb3582b09e20"
      },
      "source": [
        "evaluate_sbert(question_QA_sbert_embeddings, query_QA_sbert_embeddings_list, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######\n",
            "Query Question: how to know if i am eligible to apply for bitsat \n",
            "Retrieved Question 1: what is the eligibility for bitsat \n",
            "######\n",
            "######\n",
            "Query Question: can i get the application form via mail \n",
            "Retrieved Question 1: i am unable to access the online application  can you send the application form by email or post \n",
            "######\n",
            "######\n",
            "Query Question: what are the fees \n",
            "Retrieved Question 1: what are the tuition fees and other expenses \n",
            "######\n",
            "######\n",
            "Query Question: can i appear for bitsat 2020 if i also took bitsat 2019 \n",
            "Retrieved Question 1: i had appeared in bitsat 2019 but my marks were below the cut off marks  can i appear in bitsat 2020 \n",
            "######\n",
            "######\n",
            "Query Question: can i appear if i failed 12th\n",
            "Retrieved Question 1: how am i eligible for admission to m sc  programmes after 12th \n",
            "######\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysgFn_GpuqLV"
      },
      "source": [
        "## <font color=\"#007bff\"><b>Observation</b></font><br><a id=\"5\"></a>\n",
        "\n",
        "We achieved the best results through **SBERT embedding** representation as it is able to derive semantically meaningful sentence embeddings (semantically similar sentences are closer in vector space). "
      ]
    }
  ]
}